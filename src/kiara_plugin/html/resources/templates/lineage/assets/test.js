const lineage_data = {'module': {'generate.LDA.for.tokens_array': {'module_info': {'type_name': 'generate.LDA.for.tokens_array', 'documentation': {'description': 'Perform Latent Dirichlet Allocation on a tokenized corpus.', 'doc': 'This module computes models for a range of number of topics provided by the user.'}, 'authors': {'authors': [{'name': 'Markus Binsteiner', 'email': 'markus@frkl.io'}]}, 'context': {'references': {'source_repo': {'url': 'https://github.com/DHARPA-Project/kiara_plugin.language_processing', 'desc': 'The module package git repository.'}, 'documentation': {'url': 'https://DHARPA-Project.github.io/kiara_plugin.language_processing/', 'desc': 'The url for the module package documentation.'}}, 'tags': ['language_processing', 'LDA', 'tokens'], 'labels': {'package': 'kiara_plugin.language_processing'}}, 'python_class': {'python_class_name': 'LDAModule', 'python_module_name': 'kiara_plugin.language_processing.modules.lda', 'full_name': 'kiara_plugin.language_processing.modules.lda.LDAModule'}, 'module_src': 'class LDAModule(KiaraModule):\n    """Perform Latent Dirichlet Allocation on a tokenized corpus.\n\n    This module computes models for a range of number of topics provided by the user.\n    """\n\n    _module_type_name = "generate.LDA.for.tokens_array"\n\n    KIARA_METADATA = {\n        "tags": ["LDA", "tokens"],\n    }\n\n    def create_inputs_schema(\n        self,\n    ) -> ValueSetSchema:\n\n        inputs: Dict[str, Dict[str, Any]] = {\n            "tokens_array": {"type": "array", "doc": "The text corpus."},\n            "num_topics_min": {\n                "type": "integer",\n                "doc": "The minimal number of topics.",\n                "default": 7,\n            },\n            "num_topics_max": {\n                "type": "integer",\n                "doc": "The max number of topics.",\n                "default": 7,\n                "optional": True,\n            },\n            "compute_coherence": {\n                "type": "boolean",\n                "doc": "Whether to compute the coherence score for each model.",\n                "default": False,\n            },\n            "words_per_topic": {\n                "type": "integer",\n                "doc": "How many words per topic to put in the result model.",\n                "default": 10,\n            },\n        }\n        return inputs\n\n    def create_outputs_schema(\n        self,\n    ) -> ValueSetSchema:\n\n        outputs: Mapping[str, Mapping[str, Any]] = {\n            "topic_models": {\n                "type": "dict",\n                "doc": "A dictionary with one coherence model table for each number of topics.",\n            },\n            "coherence_table": {\n                "type": "table",\n                "doc": "Coherence details.",\n                "optional": True,\n            },\n            "coherence_map": {\n                "type": "dict",\n                "doc": "A map with the coherence value for every number of topics.",\n            },\n        }\n        return outputs\n\n    def create_model(self, corpus, num_topics: int, id2word: Mapping[str, int]):\n        from gensim.models import LdaModel\n\n        model = LdaModel(\n            corpus, id2word=id2word, num_topics=num_topics, eval_every=None\n        )\n        return model\n\n    def compute_coherence(self, model, corpus_model, id2word: Mapping[str, int]):\n\n        from gensim.models import CoherenceModel\n\n        coherencemodel = CoherenceModel(\n            model=model,\n            texts=corpus_model,\n            dictionary=id2word,\n            coherence="c_v",\n            processes=1,\n        )\n        coherence_value = coherencemodel.get_coherence()\n        return coherence_value\n\n    def assemble_coherence(self, models_dict: Mapping[int, Any], words_per_topic: int):\n\n        import pandas as pd\n        import pyarrow as pa\n\n        # Create list with topics and topic words for each number of topics\n        num_topics_list = []\n        topics_list = []\n        for (\n            num_topics,\n            model,\n        ) in models_dict.items():\n\n            num_topics_list.append(num_topics)\n            topic_print = model.print_topics(num_words=words_per_topic)\n            topics_list.append(topic_print)\n\n        df_coherence_table = pd.DataFrame(columns=["topic_id", "words", "num_topics"])\n\n        idx = 0\n        for i in range(len(topics_list)):\n            for j in range(len(topics_list[i])):\n                df_coherence_table.loc[idx] = ""\n                df_coherence_table["topic_id"].loc[idx] = j + 1\n                df_coherence_table["words"].loc[idx] = ", ".join(\n                    re.findall(r\'"(\\w+)"\', topics_list[i][j][1])\n                )\n                df_coherence_table["num_topics"].loc[idx] = num_topics_list[i]\n                idx += 1\n\n        coherence_table = pa.Table.from_pandas(df_coherence_table, preserve_index=False)\n        return coherence_table\n\n    def process(self, inputs: ValueMap, outputs: ValueMap) -> None:\n\n        from gensim import corpora\n\n        logging.getLogger("gensim").setLevel(logging.ERROR)\n        tokens_array: KiaraArray = inputs.get_value_data("tokens_array")\n        tokens = tokens_array.arrow_array.to_pylist()\n\n        words_per_topic = inputs.get_value_data("words_per_topic")\n\n        num_topics_min = inputs.get_value_data("num_topics_min")\n        num_topics_max = inputs.get_value_data("num_topics_max")\n        if not num_topics_max:\n            num_topics_max = num_topics_min\n\n        if num_topics_max < num_topics_min:\n            raise KiaraProcessingException(\n                "The max number of topics must be larger or equal to the min number of topics."\n            )\n\n        compute_coherence = inputs.get_value_data("compute_coherence")\n        id2word = corpora.Dictionary(tokens)\n        corpus = [id2word.doc2bow(text) for text in tokens]\n\n        # model = gensim.models.ldamulticore.LdaMulticore(\n        #     corpus, id2word=id2word, num_topics=num_topics, eval_every=None\n        # )\n\n        models = {}\n        model_tables = {}\n        coherence = {}\n\n        # multi_threaded = False\n        # if not multi_threaded:\n\n        for nt in range(num_topics_min, num_topics_max + 1):\n            model = self.create_model(corpus=corpus, num_topics=nt, id2word=id2word)\n            models[nt] = model\n            topic_print_model = model.print_topics(num_words=words_per_topic)\n            # dbg(topic_print_model)\n            # df = pd.DataFrame(topic_print_model, columns=["topic_id", "words"])\n            # TODO: create table directly\n            # result_table = Table.from_pandas(df)\n            model_tables[nt] = topic_print_model\n\n            if compute_coherence:\n                coherence_result = self.compute_coherence(\n                    model=model, corpus_model=tokens, id2word=id2word\n                )\n                coherence[nt] = coherence_result\n\n        # else:\n        #     def create_model(num_topics):\n        #         model = self.create_model(corpus=corpus, num_topics=num_topics, id2word=id2word)\n        #         topic_print_model = model.print_topics(num_words=30)\n        #         df = pd.DataFrame(topic_print_model, columns=["topic_id", "words"])\n        #         # TODO: create table directly\n        #         result_table = Table.from_pandas(df)\n        #         coherence_result = None\n        #         if compute_coherence:\n        #             coherence_result = self.compute_coherence(model=model, corpus_model=tokens, id2word=id2word)\n        #         return (num_topics, model, result_table, coherence_result)\n        #\n        #     executor = ThreadPoolExecutor()\n        #     results: typing.Any = executor.map(create_model, range(num_topics_min, num_topics_max+1))\n        #     executor.shutdown(wait=True)\n        #     for r in results:\n        #         models[r[0]] = r[1]\n        #         model_tables[r[0]] = r[2]\n        #         if compute_coherence:\n        #             coherence[r[0]] = r[3]\n\n        # df_coherence = pd.DataFrame(coherence.keys(), columns=["Number of topics"])\n        # df_coherence["Coherence"] = coherence.values()\n\n        if compute_coherence:\n            coherence_table = self.assemble_coherence(\n                models_dict=models, words_per_topic=words_per_topic\n            )\n        else:\n            coherence_table = None\n\n        coherence_map = {k: v.item() for k, v in coherence.items()}\n\n        outputs.set_values(\n            topic_models=model_tables,\n            coherence_table=coherence_table,\n            coherence_map=coherence_map,\n        )\n', 'config': {'python_class': {'python_class_name': 'KiaraModuleConfig', 'python_module_name': 'kiara.models.module', 'full_name': 'kiara.models.module.KiaraModuleConfig'}, 'config_values': {'constants': {'description': 'Value constants for this module.', 'type': 'object', 'value_default': {}, 'required': False}, 'defaults': {'description': 'Value defaults for this module.', 'type': 'object', 'value_default': {}, 'required': False}}}}, 'inputs': {'compute_coherence': {'type': 'boolean', 'id': '15e1c241-0af6-4fce-9298-85ba49075aa4', 'preview': 'True'}, 'num_topics_max': {'type': 'integer', 'id': '93dea097-87e2-46ee-af25-e8b3102b62f2', 'preview': '9'}, 'num_topics_min': {'type': 'integer', 'id': 'f88cbc90-c145-4c3a-a9fe-8314ffc2b6d2', 'preview': '7'}, 'tokens_array': {'type': 'array', 'id': 'ca8444de-3edc-4602-8002-e257cf6eb0b7', 'preview': 'array\t\n[\'RAGIONE\', \'ORGANO\', \'DIFESA\', "ITALIANITÀ\'", \'1\', \'vili\', \',\', \'camorristi\', \' ...\t\n[\'RAG\', \'ONE\', \'vili\', \',\', \'camorristi\', \',\', \'sicari\', \',\', \'falsari\', \'austri ...\t\n[\'RAGIONE\', \'ORGANO\', \'DIFESA\', \'ITALIANITÀ\', \'vili\', \',\', \'camorristi\', \',\', \'s ...\t\n[\'vili\', \',\', \'camorristi\', \',\', \'sicari\', \',\', \'falsari\', \'austriacanti\', \',\',  ...\t\n[\'vili\', \',\', \'camorristi\', \',\', \'sicari\', \',\', \'falsari\', \'austriacanti\', \',\',  ...\t\n[\'RAGIONA\', \'ORGANO\', \'DIFESA\', \'ITALIANITÀ\', \'vili\', \',\', \'camorristi\', \',\', \'s ...\t\n[\'RAGIONE\', \'ORGANO\', \'DIFESA\', "ITALIANITÀ\'", \'vili\', \',\', \'camorristi\', \',\', \' ...\t\n[\'RAGIONE\', \'vili\', \',\', \'1\', \'camorristi\', \',\', \'sicari\', \',\', \'falsari\', \'aust ...\t\n[\'vili\', \',\', \'camorristi\', \',\', \'sicari\', \',\', \'falsari\', \'austriacanti\', \',\',  ...\t\n[\'RAG\', \'ONE\', \'ORGANO\', \'DIFESA\', \'ITALIANITÀ\', "\'\'", \'vili\', \',\', \'camorristi\' ...\t\n[\'1\', \'vili\', \',\', \'camorristi\', \',\', \'sicari\', \',\', \'falsari\', \'austriacanti\',  ...\t\n[\'■■■\', \'Rassegna\', \'_\', \'Both\', \'Phones\', \'ANNO\', \'No\', \'.\', \'1\', \'perche\', "\'" ...\t\n[\'Rassegna\', \'Jjoth\', \'Phones\', \'ANNO\', \'No\', \'.\', \'2\', \'BASTA\', \'!\', \'...\', \'qu ...\t\n[\'Both\', \'Phones\', \'ANNO\', \'.\', \'No\', \'.\', \'2\', \'BASTA\', \'!\', \'...\', \'uà\', \'quai ...\t\n[\'■\', \'jSrìt\', \'*\', \'*\', \'W\', \'?\', \'?\', \'iIK\', \'38®f-\', \'i^M\', \'F\', \'<\', \'5É\', \' ...\t\n[\'■Both\', \'Phones\', \'ANNO\', \'11\', \'.\', \'No\', \'.\', \'5\', \'COSE\', \'POSTO\', \'va\', \'d ...\t\n', 'module': {'preprocess.tokens_array': {'module_info': {'type_name': 'preprocess.tokens_array', 'documentation': {'description': 'Preprocess lists of tokens, incl. lowercasing, remove special characers, etc.', 'doc': "Lowercasing: Lowercase the words. This operation is a double-edged sword. It can be effective at yielding potentially better results in the case of relatively small datasets or datatsets with a high percentage of OCR mistakes. For instance, if lowercasing is not performed, the algorithm will treat USA, Usa, usa, UsA, uSA, etc. as distinct tokens, even though they may all refer to the same entity. On the other hand, if the dataset does not contain such OCR mistakes, then it may become difficult to distinguish between homonyms and make interpreting the topics much harder.\n\nRemoving stopwords and words with less than three characters: Remove low information words. These are typically words such as articles, pronouns, prepositions, conjunctions, etc. which are not semantically salient. There are numerous stopword lists available for many, though not all, languages which can be easily adapted to the individual researcher's needs. Removing words with less than three characters may additionally remove many OCR mistakes. Both these operations have the dual advantage of yielding more reliable results while reducing the size of the dataset, thus in turn reducing the required processing power. This step can therefore hardly be considered optional in TM.\n\nNoise removal: Remove elements such as punctuation marks, special characters, numbers, html formatting, etc. This operation is again concerned with removing elements that may not be relevant to the text analysis and in fact interfere with it. Depending on the dataset and research question, this operation can become essential."}, 'authors': {'authors': [{'name': 'Markus Binsteiner', 'email': 'markus@frkl.io'}]}, 'context': {'references': {'source_repo': {'url': 'https://github.com/DHARPA-Project/kiara_plugin.language_processing', 'desc': 'The module package git repository.'}, 'documentation': {'url': 'https://DHARPA-Project.github.io/kiara_plugin.language_processing/', 'desc': 'The url for the module package documentation.'}}, 'tags': ['language_processing', 'tokens', 'preprocess'], 'labels': {'package': 'kiara_plugin.language_processing'}}, 'python_class': {'python_class_name': 'PreprocessModule', 'python_module_name': 'kiara_plugin.language_processing.modules.tokens', 'full_name': 'kiara_plugin.language_processing.modules.tokens.PreprocessModule'}, 'module_src': 'class PreprocessModule(KiaraModule):\n    """Preprocess lists of tokens, incl. lowercasing, remove special characers, etc.\n\n    Lowercasing: Lowercase the words. This operation is a double-edged sword. It can be effective at yielding potentially better results in the case of relatively small datasets or datatsets with a high percentage of OCR mistakes. For instance, if lowercasing is not performed, the algorithm will treat USA, Usa, usa, UsA, uSA, etc. as distinct tokens, even though they may all refer to the same entity. On the other hand, if the dataset does not contain such OCR mistakes, then it may become difficult to distinguish between homonyms and make interpreting the topics much harder.\n\n    Removing stopwords and words with less than three characters: Remove low information words. These are typically words such as articles, pronouns, prepositions, conjunctions, etc. which are not semantically salient. There are numerous stopword lists available for many, though not all, languages which can be easily adapted to the individual researcher\'s needs. Removing words with less than three characters may additionally remove many OCR mistakes. Both these operations have the dual advantage of yielding more reliable results while reducing the size of the dataset, thus in turn reducing the required processing power. This step can therefore hardly be considered optional in TM.\n\n    Noise removal: Remove elements such as punctuation marks, special characters, numbers, html formatting, etc. This operation is again concerned with removing elements that may not be relevant to the text analysis and in fact interfere with it. Depending on the dataset and research question, this operation can become essential.\n    """\n\n    _module_type_name = "preprocess.tokens_array"\n\n    KIARA_METADATA = {\n        "tags": ["tokens", "preprocess"],\n    }\n\n    def create_inputs_schema(\n        self,\n    ) -> ValueSetSchema:\n\n        return {\n            "tokens_array": {\n                "type": "array",\n                "doc": "The tokens array to pre-process.",\n            },\n            "to_lowercase": {\n                "type": "boolean",\n                "doc": "Apply lowercasing to the text.",\n                "default": False,\n            },\n            "remove_alphanumeric": {\n                "type": "boolean",\n                "doc": "Remove all tokens that include numbers (e.g. ex1ample).",\n                "default": False,\n            },\n            "remove_non_alpha": {\n                "type": "boolean",\n                "doc": "Remove all tokens that include punctuation and numbers (e.g. ex1a.mple).",\n                "default": False,\n            },\n            "remove_all_numeric": {\n                "type": "boolean",\n                "doc": "Remove all tokens that contain numbers only (e.g. 876).",\n                "default": False,\n            },\n            "remove_short_tokens": {\n                "type": "integer",\n                "doc": "Remove tokens shorter or equal to this value. If value is <= 0, no filtering will be done.",\n                "default": 0,\n            },\n            "remove_stopwords": {\n                "type": "list",\n                "doc": "Remove stopwords.",\n                "optional": True,\n            },\n        }\n\n    def create_outputs_schema(\n        self,\n    ) -> ValueSetSchema:\n\n        return {\n            "tokens_array": {\n                "type": "array",\n                "doc": "The pre-processed content, as an array of lists of strings.",\n            }\n        }\n\n    def process(self, inputs: ValueMap, outputs: ValueMap):\n\n        import polars as pl\n        import pyarrow as pa\n\n        tokens_array: KiaraArray = inputs.get_value_data("tokens_array")\n        lowercase: bool = inputs.get_value_data("to_lowercase")\n        remove_alphanumeric: bool = inputs.get_value_data("remove_alphanumeric")\n        remove_non_alpha: bool = inputs.get_value_data("remove_non_alpha")\n        remove_all_numeric: bool = inputs.get_value_data("remove_all_numeric")\n        remove_short_tokens: int = inputs.get_value_data("remove_short_tokens")\n\n        if remove_short_tokens is None:\n            remove_short_tokens = -1\n\n        _remove_stopwords = inputs.get_value_obj("remove_stopwords")\n        if _remove_stopwords.is_set:\n            stopword_list: Union[Iterable[str], None] = _remove_stopwords.data.list_data\n        else:\n            stopword_list = None\n\n        # it\'s better to have one method every token goes through, then do every test seperately for the token list\n        # because that way each token only needs to be touched once (which is more effective)\n        def check_token(token: str) -> Union[str, None]:\n\n            # remove short tokens first, since we can save ourselves all the other checks (which are more expensive)\n            assert isinstance(remove_short_tokens, int)\n            if remove_short_tokens > 0:\n                if len(token) <= remove_short_tokens:\n                    return None\n\n            _token: str = token\n            if lowercase:\n                _token = _token.lower()\n\n            if remove_non_alpha:\n                match = _token if _token.isalpha() else None\n                if match is None:\n                    return None\n\n            # if remove_non_alpha was set, we don\'t need to worry about tokens that include numbers, since they are already filtered out\n            if remove_alphanumeric and not remove_non_alpha:\n                match = _token if _token.isalnum() else None\n                if match is None:\n                    return None\n\n            # all-number tokens are already filtered out if the remove_non_alpha methods above ran\n            if remove_all_numeric and not remove_non_alpha:\n                match = None if _token.isdigit() else _token\n                if match is None:\n                    return None\n\n            if stopword_list and _token and _token.lower() in stopword_list:\n                return None\n\n            return _token\n\n        series = pl.Series(name="tokens", values=tokens_array.arrow_array)\n        result = series.apply(\n            lambda token_list: [\n                x for x in (check_token(token) for token in token_list) if x is not None\n            ]\n        )\n        result_array = result.to_arrow()\n\n        # TODO: remove this cast once the array data type can handle non-chunked arrays\n        chunked = pa.chunked_array(result_array)\n        outputs.set_values(tokens_array=chunked)\n', 'config': {'python_class': {'python_class_name': 'KiaraModuleConfig', 'python_module_name': 'kiara.models.module', 'full_name': 'kiara.models.module.KiaraModuleConfig'}, 'config_values': {'constants': {'description': 'Value constants for this module.', 'type': 'object', 'value_default': {}, 'required': False}, 'defaults': {'description': 'Value defaults for this module.', 'type': 'object', 'value_default': {}, 'required': False}}}}, 'inputs': {'remove_all_numeric': {'type': 'boolean', 'id': 'ee2da0d0-7fdc-4b10-8a49-42364ee4652b', 'preview': 'False'}, 'remove_alphanumeric': {'type': 'boolean', 'id': 'ee2da0d0-7fdc-4b10-8a49-42364ee4652b', 'preview': 'False'}, 'remove_non_alpha': {'type': 'boolean', 'id': 'ee2da0d0-7fdc-4b10-8a49-42364ee4652b', 'preview': 'False'}, 'remove_short_tokens': {'type': 'integer', 'id': '9ce6ace8-351f-46f9-913b-8d5481de1d67', 'preview': '0'}, 'remove_stopwords': {'type': 'list', 'id': 'e2740ea1-3082-488f-9809-d19979b1f539', 'preview': "list_data=['a', 'abbia', 'abbiamo', 'abbiano', 'abbiate', 'ad', 'agl', 'agli', 'ai', 'al', 'all', 'alla', 'alle', 'allo', 'anche', 'avemmo', 'avendo', 'avesse', 'avessero', 'avessi', 'avessimo', 'aveste', 'avesti', 'avete', 'aveva', 'avevamo', 'avevano', 'avevate', 'avevi', 'avevo', 'avrai', 'avranno', 'avrebbe', 'avrebbero', 'avrei', 'avremmo', 'avremo', 'avreste', 'avresti', 'avrete', 'avrà', 'avrò', 'avuta', 'avute', 'avuti', 'avuto', 'c', 'che', 'chi', 'ci', 'coi', 'col', 'come', 'con', 'contro', 'cui', 'da', 'dagl', 'dagli', 'dai', 'dal', 'dall', 'dalla', 'dalle', 'dallo', 'degl', 'degli', 'dei', 'del', 'dell', 'della', 'delle', 'dello', 'di', 'dov', 'dove', 'e', 'ebbe', 'ebbero', 'ebbi', 'ed', 'era', 'erano', 'eravamo', 'eravate', 'eri', 'ero', 'essendo', 'faccia', 'facciamo', 'facciano', 'facciate', 'faccio', 'facemmo', 'facendo', 'facesse', 'facessero', 'facessi', 'facessimo', 'faceste', 'facesti', 'faceva', 'facevamo', 'facevano', 'facevate', 'facevi', 'facevo', 'fai', 'fanno', 'farai', 'faranno', 'farebbe', 'farebbero', 'farei', 'faremmo', 'faremo', 'fareste', 'faresti', 'farete', 'farà', 'farò', 'fece', 'fecero', 'feci', 'fosse', 'fossero', 'fossi', 'fossimo', 'foste', 'fosti', 'fu', 'fui', 'fummo', 'furono', 'gli', 'ha', 'hai', 'hanno', 'ho', 'i', 'il', 'in', 'io', 'l', 'la', 'le', 'lei', 'li', 'lo', 'loro', 'lui', 'ma', 'mi', 'mia', 'mie', 'miei', 'mio', 'ne', 'negl', 'negli', 'nei', 'nel', 'nell', 'nella', 'nelle', 'nello', 'noi', 'non', 'nostra', 'nostre', 'nostri', 'nostro', 'o', 'per', 'perché', 'più', 'quale', 'quanta', 'quante', 'quanti', 'quanto', 'quella', 'quelle', 'quelli', 'quello', 'questa', 'queste', 'questi', 'questo', 'sarai', 'saranno', 'sarebbe', 'sarebbero', 'sarei', 'saremmo', 'saremo', 'sareste', 'saresti', 'sarete', 'sarà', 'sarò', 'se', 'sei', 'si', 'sia', 'siamo', 'siano', 'siate', 'siete', 'sono', 'sta', 'stai', 'stando', 'stanno', 'starai', 'staranno', 'starebbe', 'starebbero', 'starei', 'staremmo', 'staremo', 'stareste', 'staresti', 'starete', 'starà', 'starò', 'stava', 'stavamo', 'stavano', 'stavate', 'stavi', 'stavo', 'stemmo', 'stesse', 'stessero', 'stessi', 'stessimo', 'steste', 'stesti', 'stette', 'stettero', 'stetti', 'stia', 'stiamo', 'stiano', 'stiate', 'sto', 'su', 'sua', 'sue', 'sugl', 'sugli', 'sui', 'sul', 'sull', 'sulla', 'sulle', 'sullo', 'suo', 'suoi', 'ti', 'tra', 'tu', 'tua', 'tue', 'tuo', 'tuoi', 'tutti', 'tutto', 'un', 'una', 'uno', 'vi', 'voi', 'vostra', 'vostre', 'vostri', 'vostro', 'è'] item_schema={'title': 'list', 'type': 'object'} python_class=PythonClass(model_id=list, category=instance.wrapped_python_class, fields=[python_class_name, python_module_name, full_name])", 'module': {'create.stopwords_list': {'module_info': {'type_name': 'create.stopwords_list', 'documentation': {'description': 'Create a list of stopwords from one or multiple sources.', 'doc': 'This will download nltk stopwords if necessary, and merge all input lists into a single, sorted list without duplicates.'}, 'authors': {'authors': [{'name': 'Markus Binsteiner', 'email': 'markus@frkl.io'}]}, 'context': {'references': {'source_repo': {'url': 'https://github.com/DHARPA-Project/kiara_plugin.language_processing', 'desc': 'The module package git repository.'}, 'documentation': {'url': 'https://DHARPA-Project.github.io/kiara_plugin.language_processing/', 'desc': 'The url for the module package documentation.'}}, 'tags': ['language_processing'], 'labels': {'package': 'kiara_plugin.language_processing'}}, 'python_class': {'python_class_name': 'AssembleStopwordsModule', 'python_module_name': 'kiara_plugin.language_processing.modules.tokens', 'full_name': 'kiara_plugin.language_processing.modules.tokens.AssembleStopwordsModule'}, 'module_src': 'class AssembleStopwordsModule(KiaraModule):\n    """Create a list of stopwords from one or multiple sources.\n\n    This will download nltk stopwords if necessary, and merge all input lists into a single, sorted list without duplicates.\n    """\n\n    _module_type_name = "create.stopwords_list"\n\n    def create_inputs_schema(\n        self,\n    ) -> ValueSetSchema:\n\n        return {\n            "languages": {\n                "type": "list",\n                "doc": "A list of languages, will be used to retrieve language-specific stopword from nltk.",\n                "optional": True,\n            },\n            "stopword_list": {\n                "type": "list",\n                "doc": "A list of additional, custom stopwords.",\n                "optional": True,\n            },\n        }\n\n    def create_outputs_schema(\n        self,\n    ) -> ValueSetSchema:\n\n        return {\n            "stopwords_list": {\n                "type": "list",\n                "doc": "A sorted list of unique stopwords.",\n            }\n        }\n\n    def process(self, inputs: ValueMap, outputs: ValueMap):\n\n        stopwords = set()\n        _languages = inputs.get_value_obj("languages")\n\n        if _languages.is_set:\n            all_stopwords = get_stopwords()\n            languages: ListModel = _languages.data\n\n            for language in languages.list_data:\n\n                if language not in all_stopwords.fileids():\n                    raise KiaraProcessingException(\n                        f"Invalid language: {language}. Available: {\', \'.join(all_stopwords.fileids())}."\n                    )\n                stopwords.update(get_stopwords().words(language))\n\n        _stopword_lists = inputs.get_value_obj("stopword_list")\n        if _stopword_lists.is_set:\n            stopword_lists: ListModel = _stopword_lists.data\n            for stopword_list in stopword_lists.list_data:\n                if isinstance(stopword_list, str):\n                    stopwords.add(stopword_list)\n                else:\n                    stopwords.update(stopword_list)\n\n        outputs.set_value("stopwords_list", sorted(stopwords))\n', 'config': {'python_class': {'python_class_name': 'KiaraModuleConfig', 'python_module_name': 'kiara.models.module', 'full_name': 'kiara.models.module.KiaraModuleConfig'}, 'config_values': {'constants': {'description': 'Value constants for this module.', 'type': 'object', 'value_default': {}, 'required': False}, 'defaults': {'description': 'Value defaults for this module.', 'type': 'object', 'value_default': {}, 'required': False}}}}, 'inputs': {'languages': {'type': 'list', 'id': 'bf21e2c4-6a62-4054-9a2c-1c7722f9801c', 'preview': "list_data=['italian'] item_schema={'title': 'list', 'type': 'object'} python_class=PythonClass(model_id=list, category=instance.wrapped_python_class, fields=[python_class_name, python_module_name, full_name])"}, 'stopword_list': {'type': 'list', 'id': '6602b15a-2da8-4e3a-b146-9b6615fe740f', 'preview': "list_data=[] item_schema={'title': 'list', 'type': 'object'} python_class=PythonClass(model_id=list, category=instance.wrapped_python_class, fields=[python_class_name, python_module_name, full_name])"}}}}}, 'to_lowercase': {'type': 'boolean', 'id': 'ee2da0d0-7fdc-4b10-8a49-42364ee4652b', 'preview': 'False'}, 'tokens_array': {'type': 'array', 'id': '439010eb-3c04-49eb-afed-17341645312a', 'preview': 'array\t\n[\'LA\', \'RAGIONE\', \'ORGANO\', \'DI\', \'DIFESA\', \'DELLA\', "ITALIANITÀ\'", \'contro\', \'1 ...\t\n[\'LA\', \'RAG\', \'ONE\', \'contro\', \'i\', \'vili\', \',\', \'i\', \'camorristi\', \',\', \'i\', \'s ...\t\n[\'LA\', \'RAGIONE\', \'ORGANO\', \'DI\', \'DIFESA\', \'DELLA\', \'ITALIANITÀ\', \'contro\', \'i\' ...\t\n[\'contro\', \'i\', \'vili\', \',\', \'i\', \'camorristi\', \',\', \'i\', \'sicari\', \',\', \'i\', \'f ...\t\n[\'contro\', \'i\', \'vili\', \',\', \'i\', \'camorristi\', \',\', \'i\', \'sicari\', \',\', \'i\', \'f ...\t\n[\'LA\', \'RAGIONA\', \'ORGANO\', \'DI\', \'DIFESA\', \'DELLA\', \'ITALIANITÀ\', \'contro\', \'i\' ...\t\n[\'LA\', \'RAGIONE\', \'ORGANO\', \'DI\', \'DIFESA\', \'DELLA\', "ITALIANITÀ\'", \'contro\', \'i ...\t\n[\'LA\', \'RAGIONE\', \'contro\', \'i\', \'vili\', \',\', \'1\', \'camorristi\', \',\', \'i\', \'sica ...\t\n[\'contro\', \'i\', \'vili\', \',\', \'i\', \'camorristi\', \',\', \'i\', \'sicari\', \',\', \'i\', \'f ...\t\n[\'LA\', \'RAG\', \'ONE\', \'ORGANO\', \'DI\', \'DIFESA\', \'DELLA\', \'ITALIANITÀ\', "\'\'", \'con ...\t\n[\'contro\', \'1\', \'vili\', \',\', \'i\', \'camorristi\', \',\', \'i\', \'sicari\', \',\', \'i\', \'f ...\t\n[\'■■■\', \'La\', \'Rassegna\', \'_\', \'I\', \'Both\', \'Phones\', \'ANNO\', \'L\', \'No\', \'.\', \'1 ...\t\n[\'La\', \'Rassegna\', \'Jjoth\', \'Phones\', \'ANNO\', \'L\', \'No\', \'.\', \'2\', \'BASTA\', \'!\', ...\t\n[\'Both\', \'Phones\', \'ANNO\', \'I\', \'.\', \'No\', \'.\', \'2\', \'BASTA\', \'!\', \'...\', \'uà\',  ...\t\n[\'■\', \'jSrìt\', \'*\', \'*\', \'W\', \'?\', \'?\', \'iIK\', \'38®f-\', \'i^M\', \'F\', \'<\', \'5É\', \' ...\t\n[\'■Both\', \'Phones\', \'ANNO\', \'11\', \'.\', \'No\', \'.\', \'5\', \'LE\', \'COSE\', \'A\', \'POSTO ...\t\n', 'module': {'tokenize.texts_array': {'module_info': {'type_name': 'tokenize.texts_array', 'documentation': {'description': 'Split sentences into words or words into characters.', 'doc': 'In other words, this operation establishes the word boundaries (i.e., tokens) a very helpful way of finding patterns. It is also the typical step prior to stemming and lemmatization'}, 'authors': {'authors': [{'name': 'Markus Binsteiner', 'email': 'markus@frkl.io'}]}, 'context': {'references': {'source_repo': {'url': 'https://github.com/DHARPA-Project/kiara_plugin.language_processing', 'desc': 'The module package git repository.'}, 'documentation': {'url': 'https://DHARPA-Project.github.io/kiara_plugin.language_processing/', 'desc': 'The url for the module package documentation.'}}, 'tags': ['language_processing', 'tokenize', 'tokens'], 'labels': {'package': 'kiara_plugin.language_processing'}}, 'python_class': {'python_class_name': 'TokenizeTextArrayeModule', 'python_module_name': 'kiara_plugin.language_processing.modules.tokens', 'full_name': 'kiara_plugin.language_processing.modules.tokens.TokenizeTextArrayeModule'}, 'module_src': 'class TokenizeTextArrayeModule(KiaraModule):\n    """Split sentences into words or words into characters.\n    In other words, this operation establishes the word boundaries (i.e., tokens) a very helpful way of finding patterns. It is also the typical step prior to stemming and lemmatization\n    """\n\n    _module_type_name = "tokenize.texts_array"\n\n    KIARA_METADATA = {\n        "tags": ["tokenize", "tokens"],\n    }\n\n    def create_inputs_schema(\n        self,\n    ) -> ValueSetSchema:\n\n        return {\n            "texts_array": {\n                "type": "array",\n                "doc": "An array of text items to be tokenized.",\n            },\n            "tokenize_by_word": {\n                "type": "boolean",\n                "doc": "Whether to tokenize by word (default), or character.",\n                "default": True,\n            },\n        }\n\n    def create_outputs_schema(\n        self,\n    ) -> ValueSetSchema:\n\n        return {\n            "tokens_array": {\n                "type": "array",\n                "doc": "The tokenized content, as an array of lists of strings.",\n            }\n        }\n\n    def process(self, inputs: ValueMap, outputs: ValueMap):\n\n        pass\n\n        import nltk\n        import polars as pl\n        import pyarrow as pa\n\n        array: KiaraArray = inputs.get_value_data("texts_array")\n        # tokenize_by_word: bool = inputs.get_value_data("tokenize_by_word")\n\n        column: pa.ChunkedArray = array.arrow_array\n\n        # warnings.filterwarnings("ignore", category=np.VisibleDeprecationWarning)\n\n        def word_tokenize(word):\n            result = nltk.word_tokenize(word)\n            return result\n\n        series = pl.Series(name="tokens", values=column)\n        result = series.apply(word_tokenize)\n\n        result_array = result.to_arrow()\n\n        # TODO: remove this cast once the array data type can handle non-chunked arrays\n        chunked = pa.chunked_array(result_array)\n        outputs.set_values(tokens_array=chunked)\n', 'config': {'python_class': {'python_class_name': 'KiaraModuleConfig', 'python_module_name': 'kiara.models.module', 'full_name': 'kiara.models.module.KiaraModuleConfig'}, 'config_values': {'constants': {'description': 'Value constants for this module.', 'type': 'object', 'value_default': {}, 'required': False}, 'defaults': {'description': 'Value defaults for this module.', 'type': 'object', 'value_default': {}, 'required': False}}}}, 'inputs': {'texts_array': {'type': 'array', 'id': 'd5ab5fd4-6571-4821-b3c3-80d83ea50963', 'preview': 'array\t\nLA RAGIONE\t\nLA RAG ONE\t\nLA RAGIONE\t\ncontro i vili, i camorristi, i sicari, i falsari e gli austriacanti, nemici dell ...\t\ncontro i vili, i camorristi, i sicari, i falsari e gli austriacanti, nemici dell ...\t\nLA RAGIONA\t\nLA RAGIONE\t\nLA RAGIONE\t\ncontro i vili, i camorristi, i sicari, i falsari e gli austriacanti, nemici dell ...\t\nLA RAG ONE\t\ncontro 1 vili, i camorristi, i sicari, i falsari e gli austriacanti, nemici dell ...\t\n■■■\t\nLa Rassegna\t\nBoth Phones\t\n■ jSrìt** W?? iIK 38®f- i^M\t\n■Both Phones\t\n', 'module': {'table.pick.column': {'module_info': {'type_name': 'table.pick.column', 'documentation': {'description': 'Pick one column from a table, returning an array.', 'doc': None}, 'authors': {'authors': [{'name': 'Markus Binsteiner', 'email': 'markus@frkl.io'}]}, 'context': {'references': {'source_repo': {'url': 'https://github.com/DHARPA-Project/kiara_plugin.tabular', 'desc': 'The module package git repository.'}, 'documentation': {'url': 'https://DHARPA-Project.github.io/kiara_plugin.tabular/', 'desc': 'The url for the module package documentation.'}}, 'tags': ['tabular'], 'labels': {'package': 'kiara_plugin.tabular'}}, 'python_class': {'python_class_name': 'PickColumnModule', 'python_module_name': 'kiara_plugin.tabular.modules.table', 'full_name': 'kiara_plugin.tabular.modules.table.PickColumnModule'}, 'module_src': 'class PickColumnModule(KiaraModule):\n    """Pick one column from a table, returning an array."""\n\n    _module_type_name = "table.pick.column"\n    _config_cls = PickColumnModuleConfig\n\n    def create_inputs_schema(\n        self,\n    ) -> ValueMapSchema:\n\n        inputs: Dict[str, Any] = {"table": {"type": "table", "doc": "A table."}}\n        column_name = self.get_config_value("column_name")\n        if not column_name:\n            inputs["column_name"] = {\n                "type": "string",\n                "doc": "The name of the column to extract.",\n            }\n\n        return inputs\n\n    def create_outputs_schema(\n        self,\n    ) -> ValueMapSchema:\n\n        outputs: Mapping[str, Any] = {"array": {"type": "array", "doc": "The column."}}\n        return outputs\n\n    def process(self, inputs: ValueMap, outputs: ValueMap) -> None:\n\n        import pyarrow as pa\n\n        column_name: Union[str, None] = self.get_config_value("column_name")\n        if not column_name:\n            column_name = inputs.get_value_data("column_name")\n\n        if not column_name:\n            raise KiaraProcessingException(\n                "Could not cut column from table: column_name not provided or empty string."\n            )\n\n        table_value: Value = inputs.get_value_obj("table")\n        table_metadata: KiaraTableMetadata = table_value.get_property_data(\n            "metadata.table"\n        )\n        available = table_metadata.table.column_names\n\n        if column_name not in available:\n            raise KiaraProcessingException(\n                f"Invalid column name \'{column_name}\'. Available column names: {\', \'.join(available)}"\n            )\n\n        table: pa.Table = table_value.data.arrow_table\n        column = table.column(column_name)\n\n        outputs.set_value("array", column)\n', 'config': {'python_class': {'python_class_name': 'PickColumnModuleConfig', 'python_module_name': 'kiara_plugin.tabular.modules.table', 'full_name': 'kiara_plugin.tabular.modules.table.PickColumnModuleConfig'}, 'config_values': {'constants': {'description': 'Value constants for this module.', 'type': 'object', 'value_default': {}, 'required': False}, 'defaults': {'description': 'Value defaults for this module.', 'type': 'object', 'value_default': {}, 'required': False}, 'column_name': {'description': 'A hardcoded column name to cut.', 'type': 'string', 'value_default': None, 'required': False}}}}, 'inputs': {'column_name': {'type': 'string', 'id': 'c526c329-d4d5-4a62-9c12-eff17243bb18', 'preview': 'content'}, 'table': {'type': 'table', 'id': 'f39c4e00-3b2e-465a-846a-7dc4450a8f8a', 'preview': "id\trel_path\tmime_type\tsize\tcontent\tfile_name\t\n0\tLa_Ragione/sn84037024_1917-04-25_ed-1_seq-1_ocr.txt\ttext/plain\t16613\tLA RAGIONE\tsn84037024_1917-04-25_ed-1_seq-1_ocr.txt\t\n1\tLa_Ragione/sn84037024_1917-04-25_ed-2_seq-1_ocr.txt\ttext/plain\t16679\tLA RAG ONE\tsn84037024_1917-04-25_ed-2_seq-1_ocr.txt\t\n2\tLa_Ragione/sn84037024_1917-04-25_ed-3_seq-1_ocr.txt\ttext/plain\t16793\tLA RAGIONE\tsn84037024_1917-04-25_ed-3_seq-1_ocr.txt\t\n3\tLa_Ragione/sn84037024_1917-04-25_ed-4_seq-1_ocr.txt\ttext/plain\t16235\tcontro i vili, i camorristi, i sicari, i falsari e gli austriacanti, nemici della patria di e di quella d adozione.\tsn84037024_1917-04-25_ed-4_seq-1_ocr.txt\t\n4\tLa_Ragione/sn84037024_1917-05-05_ed-1_seq-1_ocr.txt\ttext/plain\t18346\tcontro i vili, i camorristi, i sicari, i falsari e gli austriacanti, nemici della patria di origine e di quella d' adozione\tsn84037024_1917-05-05_ed-1_seq-1_ocr.txt\t\n5\tLa_Ragione/sn84037024_1917-05-05_ed-2_seq-1_ocr.txt\ttext/plain\t18474\tLA RAGIONA\tsn84037024_1917-05-05_ed-2_seq-1_ocr.txt\t\n6\tLa_Ragione/sn84037024_1917-05-05_ed-3_seq-1_ocr.txt\ttext/plain\t18280\tLA RAGIONE\tsn84037024_1917-05-05_ed-3_seq-1_ocr.txt\t\n7\tLa_Ragione/sn84037024_1917-05-05_ed-4_seq-1_ocr.txt\ttext/plain\t18481\tLA RAGIONE\tsn84037024_1917-05-05_ed-4_seq-1_ocr.txt\t\n8\tLa_Ragione/sn84037024_1917-05-16_ed-1_seq-1_ocr.txt\ttext/plain\t18620\tcontro i vili, i camorristi, i sicari, i falsari e gli austriacanti, nemici della patria di origine e di quella d'adozione\tsn84037024_1917-05-16_ed-1_seq-1_ocr.txt\t\n9\tLa_Ragione/sn84037024_1917-05-16_ed-2_seq-1_ocr.txt\ttext/plain\t18698\tLA RAG ONE\tsn84037024_1917-05-16_ed-2_seq-1_ocr.txt\t\n10\tLa_Ragione/sn84037024_1917-05-16_ed-3_seq-1_ocr.txt\ttext/plain\t18540\tcontro 1 vili, i camorristi, i sicari, i falsari e gli austriacanti, nemici della patria di origine e di quella d'adozione.\tsn84037024_1917-05-16_ed-3_seq-1_ocr.txt\t\n11\tLa_Rassegna/sn84037025_1917-04-07_ed-1_seq-1_ocr.txt\ttext/plain\t19397\t■■■\tsn84037025_1917-04-07_ed-1_seq-1_ocr.txt\t\n12\tLa_Rassegna/sn84037025_1917-04-14_ed-1_seq-1_ocr.txt\ttext/plain\t20647\tLa Rassegna\tsn84037025_1917-04-14_ed-1_seq-1_ocr.txt\t\n13\tLa_Rassegna/sn84037025_1917-04-14_ed-2_seq-1_ocr.txt\ttext/plain\t20650\tBoth Phones\tsn84037025_1917-04-14_ed-2_seq-1_ocr.txt\t\n14\tLa_Rassegna/sn84037025_1917-04-21_ed-1_seq-1_ocr.txt\ttext/plain\t21017\t■ jSrìt** W?? iIK 38®f- i^M\tsn84037025_1917-04-21_ed-1_seq-1_ocr.txt\t\n15\tLa_Rassegna/sn84037025_1917-04-21_ed-2_seq-1_ocr.txt\ttext/plain\t20982\t■Both Phones\tsn84037025_1917-04-21_ed-2_seq-1_ocr.txt\t\n", 'module': {'create.table': {'module_info': {'type_name': 'create.table', 'documentation': {'description': '-- n/a --', 'doc': None}, 'authors': {'authors': [{'name': 'Markus Binsteiner', 'email': 'markus@frkl.io'}]}, 'context': {'references': {'source_repo': {'url': 'https://github.com/DHARPA-Project/kiara_plugin.tabular', 'desc': 'The module package git repository.'}, 'documentation': {'url': 'https://DHARPA-Project.github.io/kiara_plugin.tabular/', 'desc': 'The url for the module package documentation.'}}, 'tags': ['tabular'], 'labels': {'package': 'kiara_plugin.tabular'}}, 'python_class': {'python_class_name': 'CreateTableModule', 'python_module_name': 'kiara_plugin.tabular.modules.table', 'full_name': 'kiara_plugin.tabular.modules.table.CreateTableModule'}, 'module_src': 'class CreateTableModule(CreateFromModule):\n\n    _module_type_name = "create.table"\n    _config_cls = CreateTableModuleConfig\n\n    def create__table__from__file(self, source_value: Value) -> Any:\n        """Create a table from a file, trying to auto-determine the format of said file."""\n\n        from pyarrow import csv\n\n        input_file: FileModel = source_value.data\n        imported_data = None\n        errors = []\n        try:\n            imported_data = csv.read_csv(input_file.path)\n        except Exception as e:\n            errors.append(e)\n\n        if imported_data is None:\n            raise KiaraProcessingException(\n                f"Failed to import file \'{input_file.path}\'."\n            )\n\n        # import pandas as pd\n        # df = pd.read_csv(input_file.path)\n        # imported_data = pa.Table.from_pandas(df)\n\n        return KiaraTable.create_table(imported_data)\n\n    # def create__table__from__csv_file(self, source_value: Value) -> Any:\n    #     """Create a table from a csv_file value."""\n    #\n    #     from pyarrow import csv\n    #\n    #     input_file: FileModel = source_value.data\n    #     imported_data = csv.read_csv(input_file.path)\n    #\n    #     # import pandas as pd\n    #     # df = pd.read_csv(input_file.path)\n    #     # imported_data = pa.Table.from_pandas(df)\n    #\n    #     return KiaraTable.create_table(imported_data)\n\n    def create__table__from__file_bundle(self, source_value: Value) -> Any:\n        """Create a table value from a text file_bundle.\n\n        The resulting table will have (at a minimum) the following collumns:\n        - id: an auto-assigned index\n        - rel_path: the relative path of the file (from the provided base path)\n        - content: the text file content\n        """\n\n        import pyarrow as pa\n\n        bundle: FileBundle = source_value.data\n\n        columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS\n\n        ignore_errors = self.get_config_value("ignore_errors")\n        file_dict = bundle.read_text_file_contents(ignore_errors=ignore_errors)\n\n        # TODO: use chunks to save on memory\n        tabular: Dict[str, List[Any]] = {}\n        for column in columns:\n            for index, rel_path in enumerate(sorted(file_dict.keys())):\n\n                if column == "content":\n                    _value: Any = file_dict[rel_path]\n                elif column == "id":\n                    _value = index\n                elif column == "rel_path":\n                    _value = rel_path\n                else:\n                    file_model = bundle.included_files[rel_path]\n                    _value = getattr(file_model, column)\n\n                tabular.setdefault(column, []).append(_value)\n\n        table = pa.Table.from_pydict(tabular)\n        return KiaraTable.create_table(table)\n', 'config': {'python_class': {'python_class_name': 'CreateTableModuleConfig', 'python_module_name': 'kiara_plugin.tabular.modules.table', 'full_name': 'kiara_plugin.tabular.modules.table.CreateTableModuleConfig'}, 'config_values': {'constants': {'description': 'Value constants for this module.', 'type': 'object', 'value_default': {}, 'required': False}, 'defaults': {'description': 'Value defaults for this module.', 'type': 'object', 'value_default': {}, 'required': False}, 'source_type': {'description': 'The value type of the source value.', 'type': 'string', 'value_default': None, 'required': True}, 'target_type': {'description': 'The value type of the target.', 'type': 'string', 'value_default': None, 'required': True}, 'ignore_errors': {'description': 'Whether to ignore convert errors and omit the failed items.', 'type': 'boolean', 'value_default': False, 'required': False}}}}, 'inputs': {'file_bundle': {'type': 'file_bundle', 'id': '0ca4da43-b5dd-4438-945a-4c674f062047', 'preview': "File bundle 'data\n  size: 298.45 KB\n  contents:\n    - La_Ragione/sn84037024_1917-04-25_ed-1_seq-1_ocr.txt: sn84037024_1917-04-25_ed-1_seq-1_ocr.txt\n    - La_Ragione/sn84037024_1917-04-25_ed-2_seq-1_ocr.txt: sn84037024_1917-04-25_ed-2_seq-1_ocr.txt\n    - La_Ragione/sn84037024_1917-04-25_ed-3_seq-1_ocr.txt: sn84037024_1917-04-25_ed-3_seq-1_ocr.txt\n    - La_Ragione/sn84037024_1917-04-25_ed-4_seq-1_ocr.txt: sn84037024_1917-04-25_ed-4_seq-1_ocr.txt\n    - La_Ragione/sn84037024_1917-05-05_ed-1_seq-1_ocr.txt: sn84037024_1917-05-05_ed-1_seq-1_ocr.txt\n    - La_Ragione/sn84037024_1917-05-05_ed-2_seq-1_ocr.txt: sn84037024_1917-05-05_ed-2_seq-1_ocr.txt\n    - La_Ragione/sn84037024_1917-05-05_ed-3_seq-1_ocr.txt: sn84037024_1917-05-05_ed-3_seq-1_ocr.txt\n    - La_Ragione/sn84037024_1917-05-05_ed-4_seq-1_ocr.txt: sn84037024_1917-05-05_ed-4_seq-1_ocr.txt\n    - La_Ragione/sn84037024_1917-05-16_ed-1_seq-1_ocr.txt: sn84037024_1917-05-16_ed-1_seq-1_ocr.txt\n    - La_Ragione/sn84037024_1917-05-16_ed-2_seq-1_ocr.txt: sn84037024_1917-05-16_ed-2_seq-1_ocr.txt\n    - La_Ragione/sn84037024_1917-05-16_ed-3_seq-1_ocr.txt: sn84037024_1917-05-16_ed-3_seq-1_ocr.txt\n    - La_Rassegna/sn84037025_1917-04-07_ed-1_seq-1_ocr.txt: sn84037025_1917-04-07_ed-1_seq-1_ocr.txt\n    - La_Rassegna/sn84037025_1917-04-14_ed-1_seq-1_ocr.txt: sn84037025_1917-04-14_ed-1_seq-1_ocr.txt\n    - La_Rassegna/sn84037025_1917-04-14_ed-2_seq-1_ocr.txt: sn84037025_1917-04-14_ed-2_seq-1_ocr.txt\n    - La_Rassegna/sn84037025_1917-04-21_ed-1_seq-1_ocr.txt: sn84037025_1917-04-21_ed-1_seq-1_ocr.txt\n    - La_Rassegna/sn84037025_1917-04-21_ed-2_seq-1_ocr.txt: sn84037025_1917-04-21_ed-2_seq-1_ocr.txt", 'module': {'import.local.file_bundle': {'module_info': {'type_name': 'import.local.file_bundle', 'documentation': {'description': 'Import a folder (file_bundle) from the local filesystem.', 'doc': None}, 'authors': {'authors': [{'name': 'Markus Binsteiner', 'email': 'markus@frkl.io'}]}, 'context': {'references': {'source_repo': {'url': 'https://github.com/DHARPA-Project/kiara', 'desc': 'The kiara project git repository.'}, 'documentation': {'url': 'https://dharpa.org/kiara_documentation/', 'desc': 'The url for kiara documentation.'}}, 'tags': [], 'labels': {'package': 'kiara'}}, 'python_class': {'python_class_name': 'ImportLocalFileBundleModule', 'python_module_name': 'kiara.modules.included_core_modules.filesystem', 'full_name': 'kiara.modules.included_core_modules.filesystem.ImportLocalFileBundleModule'}, 'module_src': 'class ImportLocalFileBundleModule(KiaraModule):\n    """Import a folder (file_bundle) from the local filesystem."""\n\n    _module_type_name = "import.local.file_bundle"\n    _config_cls = ImportFileBundleConfig\n\n    def create_inputs_schema(\n        self,\n    ) -> ValueMapSchema:\n\n        return {\n            "path": {"type": "string", "doc": "The local path of the folder to import."}\n        }\n\n    def create_outputs_schema(\n        self,\n    ) -> ValueMapSchema:\n\n        return {\n            "file_bundle": {"type": "file_bundle", "doc": "The imported file bundle."}\n        }\n\n    def _retrieve_module_characteristics(self) -> ModuleCharacteristics:\n        return DEFAULT_NO_IDEMPOTENT_MODULE_CHARACTERISTICS\n\n    def process(self, inputs: ValueMap, outputs: ValueMap):\n\n        path = inputs.get_value_data("path")\n\n        include = self.get_config_value("include_file_types")\n        exclude = self.get_config_value("exclude_file_types")\n\n        config = FolderImportConfig(include_files=include, exclude_files=exclude)\n\n        file_bundle = FileBundle.import_folder(source=path, import_config=config)\n        outputs.set_value("file_bundle", file_bundle)\n', 'config': {'python_class': {'python_class_name': 'ImportFileBundleConfig', 'python_module_name': 'kiara.modules.included_core_modules.filesystem', 'full_name': 'kiara.modules.included_core_modules.filesystem.ImportFileBundleConfig'}, 'config_values': {'constants': {'description': 'Value constants for this module.', 'type': 'object', 'value_default': {}, 'required': False}, 'defaults': {'description': 'Value defaults for this module.', 'type': 'object', 'value_default': {}, 'required': False}, 'include_file_types': {'description': 'File types to include.', 'type': 'array', 'value_default': None, 'required': False}, 'exclude_file_types': {'description': 'File types to include.', 'type': 'array', 'value_default': None, 'required': False}}}}, 'inputs': {'path': {'type': 'string', 'id': '57827a92-9259-4be5-81a3-c26ce2e74d3f', 'preview': '/home/markus/temp/jupyter/kiara.examples/examples/pipelines/topic_modeling/../../data/text_corpus/data'}}}}}}}}}}}}}, 'tokenize_by_word': {'type': 'boolean', 'id': '15e1c241-0af6-4fce-9298-85ba49075aa4', 'preview': 'True'}}}}}}}}}, 'words_per_topic': {'type': 'integer', 'id': '4563f90d-0b84-48f7-8934-6d74aed188bd', 'preview': '10'}}}, 'output_name': 'coherence_table'}};
